2023-02-05 17:35:58.096731: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-05 17:35:58.569595: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/local/cuda-11.2/extras/CUPTI/lib64
2023-02-05 17:35:58.569645: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/local/cuda-11.2/extras/CUPTI/lib64
2023-02-05 17:35:58.569650: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
/usr/local/lib/python3.8/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2023-02-05 17:36:15.970435: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-05 17:36:16.441835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9632 MB memory:  -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
I0205 17:36:16.995007 140683168741184 main.py:39] 0
I0205 17:36:16.995128 140683168741184 main.py:40] 5
I0205 17:36:17.225956 140683168741184 main.py:86] Base GRU generated
I0205 17:36:17.226106 140683168741184 bayesian_opt.py:44] 10
I0205 17:36:17.226135 140683168741184 bayesian_opt.py:45] 10
2023-02-05 17:36:19.638281: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8201
2023-02-05 17:36:20.406241: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7ff1e80ed910 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-02-05 17:36:20.406284: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
2023-02-05 17:36:20.409655: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-02-05 17:36:20.497779: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
WARNING:tensorflow:5 out of the last 17 calls to <function Testing_routine.test_step_2 at 0x7ff2d01b2a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
W0205 17:39:31.061480 140683168741184 polymorphic_function.py:154] 5 out of the last 17 calls to <function Testing_routine.test_step_2 at 0x7ff2d01b2a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:5 out of the last 17 calls to <function Testing_routine.test_step_2 at 0x7ff26803d940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
W0205 17:41:02.409478 140683168741184 polymorphic_function.py:154] 5 out of the last 17 calls to <function Testing_routine.test_step_2 at 0x7ff26803d940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
I0205 18:02:36.252228 140683168741184 bayesian_opt.py:53] bayes done
I0205 18:02:36.252410 140683168741184 main.py:96] Bayesian optimisation done
I0205 18:02:36.252440 140683168741184 main.py:98] Starting training with best parameters
I0205 18:02:36.481574 140683168741184 main.py:110] Base GRU generated
I0205 18:03:38.487548 140683168741184 main.py:126] starting Visualization
Data Generated
train, test, val split completed
Data generated in /home/RUS_CIP/st180170/dl-lab-22w-team14/Human_Activity_Recognition/train_test_val/
|   iter    |  target   | dropou... | learni... |   units   |
-------------------------------------------------------------
| [0m 1       [0m | [0m 0.0     [0m | [0m 0.2085  [0m | [0m 0.07204 [0m | [0m 16.03   [0m |
| [95m 2       [0m | [95m 0.9251  [0m | [95m 0.1512  [0m | [95m 0.01468 [0m | [95m 38.16   [0m |
| [0m 3       [0m | [0m 0.0     [0m | [0m 0.09313 [0m | [0m 0.03456 [0m | [0m 111.2   [0m |
| [0m 4       [0m | [0m 0.7724  [0m | [0m 0.2694  [0m | [0m 0.04193 [0m | [0m 180.5   [0m |
| [0m 5       [0m | [0m 0.0     [0m | [0m 0.1022  [0m | [0m 0.08781 [0m | [0m 22.57   [0m |
| [0m 6       [0m | [0m 0.8618  [0m | [0m 0.3352  [0m | [0m 0.04174 [0m | [0m 150.1   [0m |
| [0m 7       [0m | [0m 0.0     [0m | [0m 0.07019 [0m | [0m 0.01982 [0m | [0m 208.2   [0m |
| [0m 8       [0m | [0m 0.8388  [0m | [0m 0.4841  [0m | [0m 0.03135 [0m | [0m 182.2   [0m |
| [0m 9       [0m | [0m 0.0     [0m | [0m 0.4382  [0m | [0m 0.08946 [0m | [0m 36.41   [0m |
| [0m 10      [0m | [0m 0.0     [0m | [0m 0.01953 [0m | [0m 0.01699 [0m | [0m 226.8   [0m |
| [0m 11      [0m | [0m 0.8955  [0m | [0m 0.1799  [0m | [0m 0.003718[0m | [0m 181.4   [0m |
| [0m 12      [0m | [0m 0.4051  [0m | [0m 0.0     [0m | [0m 1e-05   [0m | [0m 39.74   [0m |
| [0m 13      [0m | [0m 0.7258  [0m | [0m 0.3776  [0m | [0m 0.06008 [0m | [0m 151.5   [0m |
| [95m 14      [0m | [95m 0.9413  [0m | [95m 0.328   [0m | [95m 0.01366 [0m | [95m 148.4   [0m |
| [0m 15      [0m | [0m 0.0     [0m | [0m 0.5     [0m | [0m 0.1     [0m | [0m 146.6   [0m |
| [0m 16      [0m | [0m 0.8154  [0m | [0m 0.00346 [0m | [0m 0.0507  [0m | [0m 149.2   [0m |
| [0m 17      [0m | [0m 0.9296  [0m | [0m 0.003661[0m | [0m 0.00612 [0m | [0m 184.0   [0m |
| [0m 18      [0m | [0m 0.0     [0m | [0m 0.3494  [0m | [0m 0.09356 [0m | [0m 185.5   [0m |
| [0m 19      [0m | [0m 0.0     [0m | [0m 0.02887 [0m | [0m 0.07921 [0m | [0m 183.1   [0m |
| [0m 20      [0m | [0m 0.0     [0m | [0m 0.4719  [0m | [0m 0.07905 [0m | [0m 148.9   [0m |
=============================================================
Epoch: 0 - Train - loss:1.019413, acc: 0.557020, f1: 0.614270, recall: 0.570288, precision: 0.716520
Epoch: 0 - Val - loss:0.423598, acc: 0.827984, f1: 0.820784, recall: 0.895150, precision: 0.759544

Epoch: 1 - Val - loss:0.353207, acc: 0.851569, f1: 0.834498, recall: 0.918896, precision: 0.765316

Epoch: 2 - Val - loss:0.269271, acc: 0.898108, f1: 0.846681, recall: 0.948046, precision: 0.766118

Epoch: 3 - Val - loss:0.245676, acc: 0.898034, f1: 0.849939, recall: 0.956317, precision: 0.765914

Epoch: 4 - Val - loss:0.226763, acc: 0.897273, f1: 0.852659, recall: 0.963807, precision: 0.765517

Epoch: 5 - Val - loss:0.222041, acc: 0.895933, f1: 0.853156, recall: 0.967146, precision: 0.764266

Epoch: 6 - Val - loss:0.216432, acc: 0.902407, f1: 0.852151, recall: 0.971600, precision: 0.759936

Epoch: 7 - Val - loss:0.205170, acc: 0.906694, f1: 0.855368, recall: 0.972889, precision: 0.764410

Epoch: 8 - Val - loss:0.220737, acc: 0.881315, f1: 0.850302, recall: 0.973471, precision: 0.755709

Epoch: 9 - Val - loss:0.206668, acc: 0.897806, f1: 0.850280, recall: 0.973428, precision: 0.755881

Epoch: 10 - Val - loss:0.224595, acc: 0.867781, f1: 0.849521, recall: 0.976392, precision: 0.752751

Epoch: 11 - Val - loss:0.251284, acc: 0.889856, f1: 0.851363, recall: 0.977588, precision: 0.754968

Epoch: 12 - Val - loss:0.258403, acc: 0.875500, f1: 0.850301, recall: 0.977511, precision: 0.753381

Epoch: 13 - Val - loss:0.276946, acc: 0.867048, f1: 0.849509, recall: 0.979334, precision: 0.751097

Epoch: 14 - Val - loss:0.267888, acc: 0.876745, f1: 0.852361, recall: 0.981355, precision: 0.754383

Epoch: 15 - Val - loss:0.249390, acc: 0.883482, f1: 0.849476, recall: 0.981688, precision: 0.749627

Epoch: 16 - Val - loss:0.287914, acc: 0.876037, f1: 0.849156, recall: 0.980368, precision: 0.749818

Epoch: 17 - Val - loss:0.234546, acc: 0.882439, f1: 0.852295, recall: 0.981166, precision: 0.754416

Time taken:  60.57806181907654
Test - loss:0.154579, acc: 0.933961, f1: 0.863830, recall: 0.981429, precision: 0.771650
 1/38 [..............................] - ETA: 12s 8/38 [=====>........................] - ETA: 0s 15/38 [==========>...................] - ETA: 0s22/38 [================>.............] - ETA: 0s29/38 [=====================>........] - ETA: 0s37/38 [============================>.] - ETA: 0s38/38 [==============================] - 1s 7ms/step
I0205 18:03:41.262930 140683168741184 main.py:39] 1
I0205 18:03:41.263045 140683168741184 main.py:40] 5
I0205 18:03:41.474874 140683168741184 main.py:86] Base GRU generated
I0205 18:03:41.475030 140683168741184 bayesian_opt.py:44] 10
I0205 18:03:41.475058 140683168741184 bayesian_opt.py:45] 10
I0205 18:29:57.517299 140683168741184 bayesian_opt.py:53] bayes done
I0205 18:29:57.517490 140683168741184 main.py:96] Bayesian optimisation done
I0205 18:29:57.517521 140683168741184 main.py:98] Starting training with best parameters
I0205 18:29:57.750458 140683168741184 main.py:110] Base GRU generated
I0205 18:31:05.450675 140683168741184 main.py:126] starting Visualization


|   iter    |  target   | dropou... | learni... |   units   |
-------------------------------------------------------------
| [0m 1       [0m | [0m 0.0     [0m | [0m 0.2085  [0m | [0m 0.07204 [0m | [0m 16.03   [0m |
| [95m 2       [0m | [95m 0.9158  [0m | [95m 0.1512  [0m | [95m 0.01468 [0m | [95m 38.16   [0m |
| [95m 3       [0m | [95m 0.933   [0m | [95m 0.09313 [0m | [95m 0.03456 [0m | [95m 111.2   [0m |
| [0m 4       [0m | [0m 0.8334  [0m | [0m 0.2694  [0m | [0m 0.04193 [0m | [0m 180.5   [0m |
| [0m 5       [0m | [0m 0.0     [0m | [0m 0.1022  [0m | [0m 0.08781 [0m | [0m 22.57   [0m |
| [0m 6       [0m | [0m 0.7891  [0m | [0m 0.3352  [0m | [0m 0.04174 [0m | [0m 150.1   [0m |
| [0m 7       [0m | [0m 0.9292  [0m | [0m 0.07019 [0m | [0m 0.01982 [0m | [0m 208.2   [0m |
| [0m 8       [0m | [0m 0.8142  [0m | [0m 0.4841  [0m | [0m 0.03135 [0m | [0m 182.2   [0m |
| [0m 9       [0m | [0m 0.0     [0m | [0m 0.4382  [0m | [0m 0.08946 [0m | [0m 36.41   [0m |
| [0m 10      [0m | [0m 0.9267  [0m | [0m 0.01953 [0m | [0m 0.01699 [0m | [0m 226.8   [0m |
| [0m 11      [0m | [0m 0.0     [0m | [0m 0.1879  [0m | [0m 0.07899 [0m | [0m 201.9   [0m |
| [0m 12      [0m | [0m 0.0     [0m | [0m 0.2661  [0m | [0m 0.07874 [0m | [0m 57.98   [0m |
| [0m 13      [0m | [0m 0.0     [0m | [0m 0.2979  [0m | [0m 0.02611 [0m | [0m 195.2   [0m |
| [0m 14      [0m | [0m 0.7535  [0m | [0m 0.1138  [0m | [0m 0.01557 [0m | [0m 243.4   [0m |
| [0m 15      [0m | [0m 0.0     [0m | [0m 0.3631  [0m | [0m 0.07286 [0m | [0m 62.73   [0m |
| [95m 16      [0m | [95m 0.9356  [0m | [95m 0.3737  [0m | [95m 0.01593 [0m | [95m 111.3   [0m |
| [0m 17      [0m | [0m 0.5026  [0m | [0m 0.0     [0m | [0m 1e-05   [0m | [0m 39.85   [0m |
| [0m 18      [0m | [0m 0.0     [0m | [0m 0.003398[0m | [0m 0.0622  [0m | [0m 112.8   [0m |
| [0m 19      [0m | [0m 0.7197  [0m | [0m 0.495   [0m | [0m 0.04606 [0m | [0m 110.2   [0m |
| [0m 20      [0m | [0m 0.0     [0m | [0m 0.4468  [0m | [0m 0.09673 [0m | [0m 209.2   [0m |
=============================================================
Epoch: 0 - Train - loss:0.915840, acc: 0.575102, f1: 0.653381, recall: 0.614232, precision: 0.732978
Epoch: 0 - Val - loss:0.547823, acc: 0.749689, f1: 0.832164, recall: 0.912758, precision: 0.766007

Epoch: 1 - Val - loss:0.365520, acc: 0.854846, f1: 0.829903, recall: 0.905221, precision: 0.767163

Epoch: 2 - Val - loss:0.256803, acc: 0.899618, f1: 0.847169, recall: 0.960519, precision: 0.758736

Epoch: 3 - Val - loss:0.233493, acc: 0.904493, f1: 0.853339, recall: 0.959387, precision: 0.769590

Epoch: 4 - Val - loss:0.227490, acc: 0.901115, f1: 0.853708, recall: 0.964892, precision: 0.766632

Epoch: 5 - Val - loss:0.207723, acc: 0.921047, f1: 0.852963, recall: 0.971045, precision: 0.761453

Epoch: 6 - Val - loss:0.210565, acc: 0.910581, f1: 0.850702, recall: 0.970726, precision: 0.758206

Epoch: 7 - Val - loss:0.201509, acc: 0.912859, f1: 0.853436, recall: 0.971651, precision: 0.761983

Epoch: 8 - Val - loss:0.195242, acc: 0.906612, f1: 0.852115, recall: 0.975056, precision: 0.757674

Epoch: 9 - Val - loss:0.200880, acc: 0.897800, f1: 0.852731, recall: 0.972523, precision: 0.760351

Epoch: 10 - Val - loss:0.199423, acc: 0.911716, f1: 0.851901, recall: 0.978638, precision: 0.755197

Epoch: 11 - Val - loss:0.215525, acc: 0.875860, f1: 0.853198, recall: 0.976794, precision: 0.758306

Epoch: 12 - Val - loss:0.238818, acc: 0.888752, f1: 0.851117, recall: 0.977054, precision: 0.754785

Epoch: 13 - Val - loss:0.232042, acc: 0.888633, f1: 0.853512, recall: 0.981026, precision: 0.756195

Epoch: 14 - Val - loss:0.222184, acc: 0.889134, f1: 0.852544, recall: 0.981770, precision: 0.754300

Epoch: 15 - Val - loss:0.263981, acc: 0.881776, f1: 0.849286, recall: 0.979571, precision: 0.750585

Epoch: 16 - Val - loss:0.318005, acc: 0.880767, f1: 0.851793, recall: 0.979315, precision: 0.754681

Epoch: 17 - Val - loss:0.248194, acc: 0.885921, f1: 0.848947, recall: 0.983242, precision: 0.747989

Epoch: 18 - Val - loss:0.293265, acc: 0.870041, f1: 0.850471, recall: 0.980987, precision: 0.751612

Time taken:  66.10670733451843
Test - loss:0.159450, acc: 0.931808, f1: 0.863517, recall: 0.982549, precision: 0.770515
 1/38 [..............................] - ETA: 12s 8/38 [=====>........................] - ETA: 0s 15/38 [==========>...................] - ETA: 0s22/38 [================>.............] - ETA: 0s29/38 [=====================>........] - ETA: 0s36/38 [===========================>..] - ETA: 0s38/38 [==============================] - 1s 7ms/step
I0205 18:31:08.358167 140683168741184 main.py:39] 2
I0205 18:31:08.358294 140683168741184 main.py:40] 5
I0205 18:31:08.574510 140683168741184 main.py:86] Base GRU generated
I0205 18:31:08.574682 140683168741184 bayesian_opt.py:44] 10
I0205 18:31:08.574711 140683168741184 bayesian_opt.py:45] 10
I0205 18:56:01.524390 140683168741184 bayesian_opt.py:53] bayes done
I0205 18:56:01.524596 140683168741184 main.py:96] Bayesian optimisation done
I0205 18:56:01.524626 140683168741184 main.py:98] Starting training with best parameters
I0205 18:56:01.762755 140683168741184 main.py:110] Base GRU generated
I0205 18:57:23.730242 140683168741184 main.py:126] starting Visualization


|   iter    |  target   | dropou... | learni... |   units   |
-------------------------------------------------------------
| [0m 1       [0m | [0m 0.0     [0m | [0m 0.2085  [0m | [0m 0.07204 [0m | [0m 16.03   [0m |
| [95m 2       [0m | [95m 0.9315  [0m | [95m 0.1512  [0m | [95m 0.01468 [0m | [95m 38.16   [0m |
| [0m 3       [0m | [0m 0.67    [0m | [0m 0.09313 [0m | [0m 0.03456 [0m | [0m 111.2   [0m |
| [0m 4       [0m | [0m 0.7081  [0m | [0m 0.2694  [0m | [0m 0.04193 [0m | [0m 180.5   [0m |
| [0m 5       [0m | [0m 0.0     [0m | [0m 0.1022  [0m | [0m 0.08781 [0m | [0m 22.57   [0m |
| [0m 6       [0m | [0m 0.0     [0m | [0m 0.3352  [0m | [0m 0.04174 [0m | [0m 150.1   [0m |
| [0m 7       [0m | [0m 0.7937  [0m | [0m 0.07019 [0m | [0m 0.01982 [0m | [0m 208.2   [0m |
| [0m 8       [0m | [0m 0.849   [0m | [0m 0.4841  [0m | [0m 0.03135 [0m | [0m 182.2   [0m |
| [0m 9       [0m | [0m 0.6089  [0m | [0m 0.4382  [0m | [0m 0.08946 [0m | [0m 36.41   [0m |
| [0m 10      [0m | [0m 0.7915  [0m | [0m 0.01953 [0m | [0m 0.01699 [0m | [0m 226.8   [0m |
| [0m 11      [0m | [0m 0.0     [0m | [0m 0.08424 [0m | [0m 0.05275 [0m | [0m 42.48   [0m |
| [0m 12      [0m | [0m 0.0     [0m | [0m 0.08988 [0m | [0m 0.09641 [0m | [0m 184.8   [0m |
| [0m 13      [0m | [0m 0.0     [0m | [0m 0.4057  [0m | [0m 0.09039 [0m | [0m 39.38   [0m |
| [0m 14      [0m | [0m 0.8513  [0m | [0m 0.1138  [0m | [0m 0.01557 [0m | [0m 243.4   [0m |
| [0m 15      [0m | [0m 0.5918  [0m | [0m 0.3631  [0m | [0m 0.07286 [0m | [0m 62.73   [0m |
| [0m 16      [0m | [0m 0.8682  [0m | [0m 0.2351  [0m | [0m 0.04236 [0m | [0m 129.3   [0m |
| [0m 17      [0m | [0m 0.0     [0m | [0m 0.1025  [0m | [0m 0.04696 [0m | [0m 243.3   [0m |
| [0m 18      [0m | [0m 0.0     [0m | [0m 0.2444  [0m | [0m 0.08412 [0m | [0m 251.0   [0m |
| [0m 19      [0m | [0m 0.0     [0m | [0m 0.1612  [0m | [0m 0.07636 [0m | [0m 221.7   [0m |
| [0m 20      [0m | [0m 0.812   [0m | [0m 0.4275  [0m | [0m 0.07194 [0m | [0m 234.9   [0m |
=============================================================
Epoch: 0 - Train - loss:1.020437, acc: 0.529977, f1: 0.568259, recall: 0.494476, precision: 0.750652
Epoch: 0 - Val - loss:0.564707, acc: 0.770122, f1: 0.741833, recall: 0.724132, precision: 0.762352

Epoch: 1 - Val - loss:0.405574, acc: 0.820235, f1: 0.817223, recall: 0.892126, precision: 0.755154

Epoch: 2 - Val - loss:0.329150, acc: 0.862594, f1: 0.831339, recall: 0.920136, precision: 0.759856

Epoch: 3 - Val - loss:0.318346, acc: 0.867537, f1: 0.836483, recall: 0.933190, precision: 0.759252

Epoch: 4 - Val - loss:0.264672, acc: 0.894757, f1: 0.849975, recall: 0.947175, precision: 0.772624

Epoch: 5 - Val - loss:0.256236, acc: 0.888718, f1: 0.851750, recall: 0.952935, precision: 0.771695

Epoch: 6 - Val - loss:0.256390, acc: 0.883900, f1: 0.852607, recall: 0.954180, precision: 0.772167

Epoch: 7 - Val - loss:0.252297, acc: 0.875080, f1: 0.851660, recall: 0.958904, precision: 0.767464

Epoch: 8 - Val - loss:0.246656, acc: 0.875286, f1: 0.851938, recall: 0.958994, precision: 0.767780

Epoch: 9 - Val - loss:0.262659, acc: 0.870859, f1: 0.852915, recall: 0.962113, precision: 0.767257

Epoch: 10 - Val - loss:0.267771, acc: 0.865942, f1: 0.853893, recall: 0.963062, precision: 0.768233

Epoch: 11 - Val - loss:0.251702, acc: 0.870342, f1: 0.850494, recall: 0.967924, precision: 0.759673

Epoch: 12 - Val - loss:0.302874, acc: 0.832370, f1: 0.849899, recall: 0.963763, precision: 0.761299

Epoch: 13 - Val - loss:0.217214, acc: 0.893990, f1: 0.849906, recall: 0.970326, precision: 0.757268

Epoch: 14 - Val - loss:0.268521, acc: 0.849438, f1: 0.850560, recall: 0.971712, precision: 0.757271

Epoch: 15 - Val - loss:0.253702, acc: 0.861611, f1: 0.849397, recall: 0.972937, precision: 0.754655

Epoch: 16 - Val - loss:0.267381, acc: 0.847372, f1: 0.851099, recall: 0.971860, precision: 0.757977

Epoch: 17 - Val - loss:0.253434, acc: 0.861391, f1: 0.849107, recall: 0.974648, precision: 0.753118

Epoch: 18 - Val - loss:0.277081, acc: 0.848381, f1: 0.850576, recall: 0.975246, precision: 0.755088

Epoch: 19 - Val - loss:0.257214, acc: 0.857446, f1: 0.850939, recall: 0.972088, precision: 0.757715

Epoch: 20 - Train - loss:0.202869, acc: 0.901070, f1: 0.839920, recall: 0.968894, precision: 0.743427
Epoch: 20 - Val - loss:0.244776, acc: 0.874533, f1: 0.850560, recall: 0.969556, precision: 0.758736

Epoch: 21 - Val - loss:0.268583, acc: 0.857190, f1: 0.850636, recall: 0.977247, precision: 0.754111

Epoch: 22 - Val - loss:0.322276, acc: 0.849749, f1: 0.850838, recall: 0.979331, precision: 0.753191

Epoch: 23 - Val - loss:0.231499, acc: 0.876512, f1: 0.851123, recall: 0.978778, precision: 0.753897

Time taken:  80.36581921577454
Test - loss:0.183809, acc: 0.923973, f1: 0.861974, recall: 0.979313, precision: 0.770037
 1/38 [..............................] - ETA: 12s 9/38 [======>.......................] - ETA: 0s 18/38 [=============>................] - ETA: 0s27/38 [====================>.........] - ETA: 0s36/38 [===========================>..] - ETA: 0s38/38 [==============================] - 1s 6ms/step
I0205 18:57:26.673360 140683168741184 main.py:39] 3
I0205 18:57:26.673481 140683168741184 main.py:40] 5
I0205 18:57:26.917473 140683168741184 main.py:86] Base GRU generated
I0205 18:57:26.917678 140683168741184 bayesian_opt.py:44] 10
I0205 18:57:26.917707 140683168741184 bayesian_opt.py:45] 10
I0205 19:28:03.569000 140683168741184 bayesian_opt.py:53] bayes done
I0205 19:28:03.569180 140683168741184 main.py:96] Bayesian optimisation done
I0205 19:28:03.569209 140683168741184 main.py:98] Starting training with best parameters
I0205 19:28:03.848351 140683168741184 main.py:110] Base GRU generated
I0205 19:29:46.480714 140683168741184 main.py:126] starting Visualization


|   iter    |  target   | dropou... | learni... |   units   |
-------------------------------------------------------------
| [0m 1       [0m | [0m 0.8801  [0m | [0m 0.2085  [0m | [0m 0.07204 [0m | [0m 16.03   [0m |
| [95m 2       [0m | [95m 0.9027  [0m | [95m 0.1512  [0m | [95m 0.01468 [0m | [95m 38.16   [0m |
| [0m 3       [0m | [0m 0.0     [0m | [0m 0.09313 [0m | [0m 0.03456 [0m | [0m 111.2   [0m |
| [0m 4       [0m | [0m 0.8036  [0m | [0m 0.2694  [0m | [0m 0.04193 [0m | [0m 180.5   [0m |
| [0m 5       [0m | [0m 0.0     [0m | [0m 0.1022  [0m | [0m 0.08781 [0m | [0m 22.57   [0m |
| [0m 6       [0m | [0m 0.8212  [0m | [0m 0.3352  [0m | [0m 0.04174 [0m | [0m 150.1   [0m |
| [95m 7       [0m | [95m 0.9214  [0m | [95m 0.07019 [0m | [95m 0.01982 [0m | [95m 208.2   [0m |
| [0m 8       [0m | [0m 0.0     [0m | [0m 0.4841  [0m | [0m 0.03135 [0m | [0m 182.2   [0m |
| [0m 9       [0m | [0m 0.0     [0m | [0m 0.4382  [0m | [0m 0.08946 [0m | [0m 36.41   [0m |
| [0m 10      [0m | [0m 0.0     [0m | [0m 0.01953 [0m | [0m 0.01699 [0m | [0m 226.8   [0m |
| [0m 11      [0m | [0m 0.0     [0m | [0m 0.1879  [0m | [0m 0.07899 [0m | [0m 201.9   [0m |
| [0m 12      [0m | [0m 0.7411  [0m | [0m 0.2661  [0m | [0m 0.07874 [0m | [0m 57.98   [0m |
| [0m 13      [0m | [0m 0.9195  [0m | [0m 0.2979  [0m | [0m 0.02611 [0m | [0m 195.2   [0m |
| [0m 14      [0m | [0m 0.8976  [0m | [0m 0.1138  [0m | [0m 0.01557 [0m | [0m 243.4   [0m |
| [0m 15      [0m | [0m 0.0     [0m | [0m 0.3631  [0m | [0m 0.07286 [0m | [0m 62.73   [0m |
| [0m 16      [0m | [0m 0.8478  [0m | [0m 0.2351  [0m | [0m 0.04236 [0m | [0m 129.3   [0m |
| [0m 17      [0m | [0m 0.8017  [0m | [0m 0.1025  [0m | [0m 0.04696 [0m | [0m 243.3   [0m |
| [0m 18      [0m | [0m 0.0     [0m | [0m 0.2444  [0m | [0m 0.08412 [0m | [0m 251.0   [0m |
| [0m 19      [0m | [0m 0.836   [0m | [0m 0.3105  [0m | [0m 1e-05   [0m | [0m 243.5   [0m |
| [0m 20      [0m | [0m 0.8552  [0m | [0m 0.1732  [0m | [0m 0.05772 [0m | [0m 207.9   [0m |
=============================================================
Epoch: 0 - Train - loss:1.192245, acc: 0.506044, f1: 0.634933, recall: 0.612306, precision: 0.702051
Epoch: 0 - Val - loss:0.566485, acc: 0.739876, f1: 0.821084, recall: 0.885993, precision: 0.766366

Epoch: 1 - Val - loss:0.475105, acc: 0.771242, f1: 0.819787, recall: 0.886436, precision: 0.763550

Epoch: 2 - Val - loss:0.360498, acc: 0.833744, f1: 0.838622, recall: 0.929178, precision: 0.765666

Epoch: 3 - Val - loss:0.310424, acc: 0.853475, f1: 0.839893, recall: 0.938079, precision: 0.761548

Epoch: 4 - Val - loss:0.296524, acc: 0.867120, f1: 0.844805, recall: 0.946506, precision: 0.764013

Epoch: 5 - Val - loss:0.282843, acc: 0.882354, f1: 0.847272, recall: 0.955493, precision: 0.762288

Epoch: 6 - Val - loss:0.275309, acc: 0.878062, f1: 0.846791, recall: 0.959322, precision: 0.759024

Epoch: 7 - Val - loss:0.283988, acc: 0.879882, f1: 0.844910, recall: 0.958468, precision: 0.756504

Epoch: 8 - Val - loss:0.280938, acc: 0.886382, f1: 0.849811, recall: 0.964584, precision: 0.760546

Epoch: 9 - Val - loss:0.291637, acc: 0.879389, f1: 0.846069, recall: 0.965831, precision: 0.753618

Epoch: 10 - Val - loss:0.276758, acc: 0.874643, f1: 0.849064, recall: 0.970193, precision: 0.755762

Epoch: 11 - Val - loss:0.277356, acc: 0.889801, f1: 0.847666, recall: 0.975563, precision: 0.750411

Epoch: 12 - Val - loss:0.258315, acc: 0.886065, f1: 0.846388, recall: 0.972322, precision: 0.750380

Epoch: 13 - Val - loss:0.261188, acc: 0.887456, f1: 0.850900, recall: 0.977799, precision: 0.754193

Epoch: 14 - Val - loss:0.258455, acc: 0.886732, f1: 0.847012, recall: 0.976249, precision: 0.749023

Epoch: 15 - Val - loss:0.276413, acc: 0.880391, f1: 0.847911, recall: 0.971073, precision: 0.753681

Epoch: 16 - Val - loss:0.256130, acc: 0.888053, f1: 0.846693, recall: 0.975490, precision: 0.748881

Epoch: 17 - Val - loss:0.261673, acc: 0.887002, f1: 0.847223, recall: 0.978595, precision: 0.747928

Epoch: 18 - Val - loss:0.252081, acc: 0.889568, f1: 0.850010, recall: 0.978149, precision: 0.752476

Epoch: 19 - Val - loss:0.247964, acc: 0.900390, f1: 0.847281, recall: 0.983414, precision: 0.745145

Epoch: 20 - Train - loss:0.152390, acc: 0.929717, f1: 0.842417, recall: 0.982013, precision: 0.739708
Epoch: 20 - Val - loss:0.260134, acc: 0.882343, f1: 0.848861, recall: 0.980951, precision: 0.749065

Epoch: 21 - Val - loss:0.280820, acc: 0.882986, f1: 0.846144, recall: 0.982089, precision: 0.744125

Epoch: 22 - Val - loss:0.286402, acc: 0.869742, f1: 0.845024, recall: 0.978705, precision: 0.744435

Epoch: 23 - Val - loss:0.301775, acc: 0.865974, f1: 0.844261, recall: 0.979045, precision: 0.742899

Epoch: 24 - Val - loss:0.282609, acc: 0.878937, f1: 0.845852, recall: 0.979165, precision: 0.745371

Epoch: 25 - Val - loss:0.269365, acc: 0.882585, f1: 0.848821, recall: 0.982744, precision: 0.747889

Epoch: 26 - Val - loss:0.300779, acc: 0.871442, f1: 0.847909, recall: 0.981083, precision: 0.747414

Epoch: 27 - Val - loss:0.338480, acc: 0.856445, f1: 0.844775, recall: 0.979169, precision: 0.743664

Epoch: 28 - Val - loss:0.388781, acc: 0.846751, f1: 0.846848, recall: 0.980219, precision: 0.746201

Epoch: 29 - Val - loss:0.322310, acc: 0.880105, f1: 0.846635, recall: 0.980947, precision: 0.745489

Time taken:  101.02641940116882
Test - loss:0.193745, acc: 0.921809, f1: 0.860598, recall: 0.982822, precision: 0.765673
 1/38 [..............................] - ETA: 12s 7/38 [====>.........................] - ETA: 0s 13/38 [=========>....................] - ETA: 0s20/38 [==============>...............] - ETA: 0s27/38 [====================>.........] - ETA: 0s34/38 [=========================>....] - ETA: 0s38/38 [==============================] - 1s 8ms/step
I0205 19:29:49.156265 140683168741184 main.py:39] 4
I0205 19:29:49.156396 140683168741184 main.py:40] 5
I0205 19:29:49.373238 140683168741184 main.py:86] Base GRU generated
I0205 19:29:49.373405 140683168741184 bayesian_opt.py:44] 10
I0205 19:29:49.373433 140683168741184 bayesian_opt.py:45] 10
I0205 19:54:20.898559 140683168741184 bayesian_opt.py:53] bayes done
I0205 19:54:20.898741 140683168741184 main.py:96] Bayesian optimisation done
I0205 19:54:20.898771 140683168741184 main.py:98] Starting training with best parameters
I0205 19:54:21.154297 140683168741184 main.py:110] Base GRU generated
I0205 19:55:24.343310 140683168741184 main.py:126] starting Visualization


|   iter    |  target   | dropou... | learni... |   units   |
-------------------------------------------------------------
| [0m 1       [0m | [0m 0.908   [0m | [0m 0.2085  [0m | [0m 0.07204 [0m | [0m 16.03   [0m |
| [95m 2       [0m | [95m 0.9309  [0m | [95m 0.1512  [0m | [95m 0.01468 [0m | [95m 38.16   [0m |
| [0m 3       [0m | [0m 0.9231  [0m | [0m 0.09313 [0m | [0m 0.03456 [0m | [0m 111.2   [0m |
| [0m 4       [0m | [0m 0.0     [0m | [0m 0.2694  [0m | [0m 0.04193 [0m | [0m 180.5   [0m |
| [0m 5       [0m | [0m 0.0     [0m | [0m 0.1022  [0m | [0m 0.08781 [0m | [0m 22.57   [0m |
| [0m 6       [0m | [0m 0.7776  [0m | [0m 0.3352  [0m | [0m 0.04174 [0m | [0m 150.1   [0m |
| [0m 7       [0m | [0m 0.8421  [0m | [0m 0.07019 [0m | [0m 0.01982 [0m | [0m 208.2   [0m |
| [0m 8       [0m | [0m 0.7483  [0m | [0m 0.4841  [0m | [0m 0.03135 [0m | [0m 182.2   [0m |
| [0m 9       [0m | [0m 0.0     [0m | [0m 0.4382  [0m | [0m 0.08946 [0m | [0m 36.41   [0m |
| [0m 10      [0m | [0m 0.6388  [0m | [0m 0.01953 [0m | [0m 0.01699 [0m | [0m 226.8   [0m |
| [0m 11      [0m | [0m 0.0     [0m | [0m 0.1879  [0m | [0m 0.07899 [0m | [0m 201.9   [0m |
| [0m 12      [0m | [0m 0.0     [0m | [0m 0.2661  [0m | [0m 0.07874 [0m | [0m 57.98   [0m |
| [0m 13      [0m | [0m 0.8451  [0m | [0m 0.2979  [0m | [0m 0.02611 [0m | [0m 195.2   [0m |
| [0m 14      [0m | [0m 0.8541  [0m | [0m 0.1138  [0m | [0m 0.01557 [0m | [0m 243.4   [0m |
| [0m 15      [0m | [0m 0.0     [0m | [0m 0.3631  [0m | [0m 0.07286 [0m | [0m 62.73   [0m |
| [0m 16      [0m | [0m 0.8196  [0m | [0m 0.2351  [0m | [0m 0.04236 [0m | [0m 129.3   [0m |
| [0m 17      [0m | [0m 0.8674  [0m | [0m 0.1025  [0m | [0m 0.04696 [0m | [0m 243.3   [0m |
| [0m 18      [0m | [0m 0.9247  [0m | [0m 0.1988  [0m | [0m 0.01533 [0m | [0m 39.15   [0m |
| [0m 19      [0m | [0m 0.0     [0m | [0m 0.0     [0m | [0m 0.1     [0m | [0m 242.0   [0m |
| [0m 20      [0m | [0m 0.0     [0m | [0m 0.1727  [0m | [0m 0.03209 [0m | [0m 112.2   [0m |
=============================================================
Epoch: 0 - Train - loss:1.001187, acc: 0.544185, f1: 0.535198, recall: 0.455109, precision: 0.740967
Epoch: 0 - Val - loss:0.586937, acc: 0.737391, f1: 0.764359, recall: 0.781202, precision: 0.749544

Epoch: 1 - Val - loss:0.473439, acc: 0.798415, f1: 0.804158, recall: 0.860520, precision: 0.755783

Epoch: 2 - Val - loss:0.338324, acc: 0.865333, f1: 0.837030, recall: 0.925243, precision: 0.765457

Epoch: 3 - Val - loss:0.293340, acc: 0.876638, f1: 0.847470, recall: 0.932622, precision: 0.777879

Epoch: 4 - Val - loss:0.264081, acc: 0.891459, f1: 0.851167, recall: 0.940898, precision: 0.778431

Epoch: 5 - Val - loss:0.247136, acc: 0.896456, f1: 0.853916, recall: 0.948543, precision: 0.777617

Epoch: 6 - Val - loss:0.246352, acc: 0.884104, f1: 0.850307, recall: 0.949650, precision: 0.770992

Epoch: 7 - Val - loss:0.243124, acc: 0.880636, f1: 0.851534, recall: 0.954508, precision: 0.769680

Epoch: 8 - Val - loss:0.245223, acc: 0.879007, f1: 0.849274, recall: 0.957168, precision: 0.764349

Epoch: 9 - Val - loss:0.249539, acc: 0.879579, f1: 0.850329, recall: 0.960992, precision: 0.763597

Epoch: 10 - Val - loss:0.269734, acc: 0.863888, f1: 0.847513, recall: 0.962282, precision: 0.758177

Epoch: 11 - Val - loss:0.320783, acc: 0.840507, f1: 0.847665, recall: 0.956004, precision: 0.762715

Epoch: 12 - Val - loss:0.329331, acc: 0.838808, f1: 0.840429, recall: 0.945304, precision: 0.758312

Epoch: 13 - Val - loss:0.273402, acc: 0.852821, f1: 0.848218, recall: 0.961259, precision: 0.760143

Epoch: 14 - Val - loss:0.390809, acc: 0.795652, f1: 0.847802, recall: 0.957823, precision: 0.761478

Epoch: 15 - Val - loss:0.261741, acc: 0.866472, f1: 0.848471, recall: 0.964877, precision: 0.758255

Epoch: 16 - Val - loss:0.279433, acc: 0.846791, f1: 0.846898, recall: 0.966488, precision: 0.754605

Epoch: 17 - Val - loss:0.291598, acc: 0.850879, f1: 0.845674, recall: 0.960821, precision: 0.756491

Time taken:  61.54695510864258
Test - loss:0.198065, acc: 0.906989, f1: 0.863520, recall: 0.972422, precision: 0.776895
 1/38 [..............................] - ETA: 12s10/38 [======>.......................] - ETA: 0s 19/38 [==============>...............] - ETA: 0s28/38 [=====================>........] - ETA: 0s38/38 [==============================] - ETA: 0s38/38 [==============================] - 1s 6ms/step


